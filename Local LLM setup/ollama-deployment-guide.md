# Setting Up Ollama as a Local LLM Server
